subsample = c(0.7, 0.8, 1),
gamma = 0,
colsample_bytree = 1,
min_child_weight = 1
)
set.seed(38036)
xgb_caret <- train(
x = X_train,
y = y_train,
method = "xgbTree",
trControl = ctrl,
tuneGrid = grid,
metric = "ROC"
)
print(xgb_caret$bestTune)
plot(xgb_caret)
pred_prob <- predict(xgb_caret, newdata = X_test, type = "prob")
pred_yes <- pred_prob[,"yes"]
pred_label <- ifelse(pred_yes > 0.5, 1, 0)
accuracy <- mean(pred_label == y_test)
print(accuracy)
cm <- confusionMatrix(factor(pred_label), factor(y_test), positive = "1")
pred_prob
pred_prob <- predict(xgb_caret, newdata = X_test, type = "prob")
pred_label <- ifelse(pred_prob > 0.5, 1, 0)
accuracy <- mean(pred_label == y_test)
print(accuracy)
pred_label
predictions <- predict(xgb_caret, newdata = X_test)
accuracy <- mean(predictions == y_test)
print(accuracy)
grid <- expand.grid(
nrounds = c(50, 100),
max_depth = c(3, 6, 9),
eta = c(0.01, 0.1, 0.3),
gamma = 0,
colsample_bytree = c(0.7, 1),
min_child_weight = 1,
subsample = c(0.7, 1)
)
set.seed(38036)
xgb_caret <- train(
x = X_train,
y = y_train,
method = "xgbTree",
trControl = ctrl,
tuneGrid = grid,
metric = "ROC"
)
print(xgb_caret$bestTune)
plot(xgb_caret)
predictions <- predict(xgb_caret, newdata = X_test)
accuracy <- mean(predictions == y_test)
print(accuracy)
# Predict probabilities on test data
xgb_pred_probs_test <- predict(xgb_caret, newdata = test_data2, type = "prob")[, "yes"]
# Convert probabilities to class labels
xgb_pred_class_test <- ifelse(xgb_pred_probs_test > 0.5, "yes", "no")
# Calculate accuracy
xgb_test_accuracy <- mean(xgb_pred_class_test == test_data2$amphet_use)
cat("XGBoost Test Accuracy:", round(xgb_test_accuracy, 3), "\n")
# Predict probabilities on training data
xgb_pred_probs_train <- predict(xgb_caret, newdata = train_data2, type = "prob")[, "yes"]
xgb_pred_class_train <- ifelse(xgb_pred_probs_train > 0.5, "yes", "no")
xgb_train_accuracy <- mean(xgb_pred_class_train == train_data2$amphet_use)
cat("XGBoost Training Accuracy:", round(xgb_train_accuracy, 3), "\n")
cm <- confusionMatrix(factor(xgb_pred_class_test, levels = c("no", "yes")),
factor(y_test, levels = c("no", "yes")),
positive = "yes")
print(cm)
# Convert to dataframe for ggplot
cm_table <- as.data.frame(cm$table)
colnames(cm_table) <- c("Predicted", "Actual", "Freq")
ggplot(data = cm_table, aes(x = Actual, y = Predicted, fill = Freq)) +
geom_tile(color = "white") +
geom_text(aes(label = Freq), size = 6) +
scale_fill_gradient(low = "white", high = "steelblue") +
labs(title = "XGBoost Confusion Matrix", x = "Actual", y = "Predicted") +
theme_minimal()
true_labels <- as.numeric(test_data2$amphet_use == "yes") # 1 for "yes", 0 for "no"
# ROC analysis
xgb_roc_obj <- roc(response = true_labels, predictor = xgb_pred_probs_test)
# Print AUC
xgb_auc <- auc(xgb_roc_obj)
print(xgb_auc)
# Plot ROC curve
plot(xgb_roc_obj, main = "XGBoost ROC Curve", col = "red")
setwd("~/GitHub/ST310_project")
library(caret)
library(ggplot2)
library(xgboost)
library(caret)
library(Matrix)
library(ROCR)
library(tidymodels)
library(dplyr)
library(yardstick)
library(smotefamily)
library(pROC)
library(lightgbm)
library(ROSE)
library(tibble)
data <- read.csv("data/drug_consumption.csv")
#data <- read.csv("~/Desktop/ST310 Machine Learning/project/drug_consumption.csv")
# Keep only selected drug variables along with the original features
selected_vars <- c("age", "gender", "education", "country", "ethnicity", "nscore",
"escore", "oscore","ascore", "cscore", "impuslive", "ss",
"alcohol", "choc", "cannabis", "caff", "nicotine", "amphet")
drug_data <-  data[, selected_vars]
# Rename 'impuslive' to 'impulsive'
colnames(drug_data)[colnames(drug_data) == "impuslive"] <- "impulsive"
head(drug_data)
table(drug_data$amphet)
drug_data2 <- drug_data
drug_vars <- c("alcohol", "choc", "cannabis", "caff", "nicotine", "amphet")
# Convert all to ordered numeric levels
for (var in drug_vars) {
drug_data2[[paste0(var, "_level")]] <- as.numeric(factor(drug_data2[[var]],
levels = c("CL0", "CL1", "CL2", "CL3", "CL4", "CL5", "CL6"))) - 1
}
# Remove the original categorical drug columns
drug_data2 <- drug_data2[ , !(names(drug_data2) %in% drug_vars)]
# Create binary target: 0 = no use, 1 = used
drug_data2$amphet_use <- ifelse(drug_data2$amphet_level > 0, 1, 0)
drug_data2$amphet_use <- as.factor(drug_data2$amphet_use)
# Remove amphet_level
drug_data2$amphet_level <- NULL
head(drug_data2)
set.seed(38036)
train_indices <- createDataPartition(drug_data2$amphet_use, p = 0.8, list = FALSE)
train_data2 <- drug_data2[train_indices, ]
test_data2  <- drug_data2[-train_indices, ]
head(train_data2)
table(train_data2$amphet_use)
train_data2$amphet_use <- factor(train_data2$amphet_use, levels = c(0, 1), labels = c("no", "yes"))
test_data2$amphet_use  <- factor(test_data2$amphet_use,  levels = c(0, 1), labels = c("no", "yes"))
X_train <- train_data2[, setdiff(names(train_data2), "amphet_use")]
X_test <- test_data2[, setdiff(names(test_data2), "amphet_use")]
y_train <- train_data2$amphet_use
y_test  <- test_data2$amphet_use
ctrl <- trainControl(
method = "cv",
number = 5,
verboseIter = TRUE,
classProbs = TRUE,
summaryFunction = twoClassSummary
)
grid <- expand.grid(
nrounds = c(50, 100),
max_depth = c(3, 6, 9),
eta = c(0.01, 0.1, 0.3),
gamma = 0,
colsample_bytree = c(0.7, 1),
min_child_weight = 1,
subsample = c(0.7, 1)
)
set.seed(38036)
xgb_caret <- train(
x = X_train,
y = y_train,
method = "xgbTree",
trControl = ctrl,
tuneGrid = grid,
metric = "ROC"
)
# Predict probabilities on test data
xgb_pred_probs_test <- predict(xgb_caret, newdata = test_data2, type = "prob")[, "yes"]
xgb_pred_class_test <- ifelse(xgb_pred_probs_test > 0.5, "yes", "no")
xgb_test_accuracy <- mean(xgb_pred_class_test == test_data2$amphet_use)
cat("XGBoost Test Accuracy:", round(xgb_test_accuracy, 3), "\n")
# Predict probabilities on training data
xgb_pred_probs_train <- predict(xgb_caret, newdata = train_data2, type = "prob")[, "yes"]
xgb_pred_class_train <- ifelse(xgb_pred_probs_train > 0.5, "yes", "no")
xgb_train_accuracy <- mean(xgb_pred_class_train == train_data2$amphet_use)
cat("XGBoost Training Accuracy:", round(xgb_train_accuracy, 3), "\n")
xgb_recipe <- recipe(amphet_use ~ ., data = train_data2) %>% prep()
boost <-   boost_tree(trees = tune(),
learn_rate = tune()) %>%
set_mode("classification") %>%
set_engine("xgboost", objective = "multi:softprob")
workflow_boost <- workflow() %>%
add_recipe(recipe) %>%
add_model(boost)
recipe <- recipe(amphet_use ~ ., data = train_data2) %>% prep()
boost <-   boost_tree(trees = tune(),
learn_rate = tune()) %>%
set_mode("classification") %>%
set_engine("xgboost", objective = "multi:softprob")
workflow_boost <- workflow() %>%
add_recipe(recipe) %>%
add_model(boost)
recipe <- recipe(amphet_use ~ ., data = train_data2)
boost <-   boost_tree(trees = tune(),
learn_rate = tune()) %>%
set_mode("classification") %>%
set_engine("xgboost", objective = "multi:softprob")
workflow_boost <- workflow() %>%
add_recipe(recipe) %>%
add_model(boost)
fit_boost  <- tune_grid(
workflow_boost,
cv,
metrics = metric_set(roc_auc)
)
recipe <- recipe(amphet_use ~ ., data = train_data2)
cv <- vfold_cv(train_data2, v = 10, strata = amphet_use)
boost <-   boost_tree(trees = tune(),
learn_rate = tune()) %>%
set_mode("classification") %>%
set_engine("xgboost", objective = "multi:softprob")
workflow_boost <- workflow() %>%
add_recipe(recipe) %>%
add_model(boost)
fit_boost  <- tune_grid(
workflow_boost,
cv,
metrics = metric_set(roc_auc)
)
recipe <- recipe(amphet_use ~ ., data = train_data2)
cv <- vfold_cv(train_data2, v = 10, strata = amphet_use)
boost <-   boost_tree(trees = tune(),
learn_rate = tune()) %>%
set_mode("classification") %>%
set_engine("xgboost", objective = "binary:logistic")
workflow_boost <- workflow() %>%
add_recipe(recipe) %>%
add_model(boost)
fit_boost  <- tune_grid(
workflow_boost,
cv,
metrics = metric_set(roc_auc)
)
boost_best <- fit_boost %>%
select_best() # best tuning parameters
boost_final <- finalize_model(
boost,
boost_best
)
boost_final
boost_test <- workflow_boost %>%
update_model(boost_final) %>%
last_fit(split = split) %>%
collect_metrics()
boost_test <- workflow_boost %>%
update_model(boost_final) %>%
fit(data = test_data2) %>%
collect_metrics()
final_fit <- workflow %>%
update_model(boost_model) %>%
fit(data = train_data2)
final_fit <- workflow_boost %>%
update_model(boost_model) %>%
fit(data = train_data2)
final_fit <- workflow_boost %>%
update_model(boost_final) %>%
fit(data = train_data2)
preds <- predict(final_fit, new_data = test_data2, type = "class") %>%
bind_cols(test_data2 %>% select(target))
final_fit <- workflow_boost %>%
update_model(boost_final) %>%
fit(data = train_data2)
preds <- predict(final_fit, new_data = test_data2, type = "class") %>%
bind_cols(test_data2 %>% select(amphet_use))
metrics(preds, truth = target, estimate = .pred_class)
final_fit <- workflow_boost %>%
update_model(boost_final) %>%
fit(data = train_data2)
preds <- predict(final_fit, new_data = test_data2, type = "class") %>%
bind_cols(test_data2 %>% select(amphet_use))
metrics(preds, truth = amphet_use, estimate = .pred_class)
recipe <- recipe(amphet_use ~ ., data = train_data2)
cv <- vfold_cv(train_data2, v = 5, strata = amphet_use)
boost <- boost_tree(
trees = tune(),
learn_rate = tune(),
tree_depth = tune(),
loss_reduction = tune(),
min_n = tune(),
sample_size = tune(),
mtry = tune()
) %>%
set_mode("classification") %>%
set_engine("xgboost", objective = "binary:logistic")
workflow_boost <- workflow() %>%
add_recipe(recipe) %>%
add_model(boost)
fit_boost  <- tune_grid(
workflow_boost,
cv,
metrics = metric_set(roc_auc)
)
recipe <- recipe(amphet_use ~ ., data = train_data2)
cv <- vfold_cv(train_data2, v = 5, strata = amphet_use)
boost <- boost_tree(
trees = tune(),
learn_rate = tune(),
tree_depth = tune(),
loss_reduction = tune(),
min_n = tune(),
sample_size = tune(),
) %>%
set_mode("classification") %>%
set_engine("xgboost", objective = "binary:logistic")
workflow_boost <- workflow() %>%
add_recipe(recipe) %>%
add_model(boost)
fit_boost  <- tune_grid(
workflow_boost,
cv,
metrics = metric_set(roc_auc)
)
boost_best <- fit_boost %>%
select_best() # best tuning parameters
boost_final <- finalize_model(
boost,
boost_best
)
boost_final
final_fit <- workflow_boost %>%
update_model(boost_final) %>%
fit(data = train_data2)
preds <- predict(final_fit, new_data = test_data2, type = "class") %>%
bind_cols(test_data2 %>% select(amphet_use))
metrics(preds, truth = amphet_use, estimate = .pred_class)
recipe <- recipe(amphet_use ~ ., data = train_data2)
cv <- vfold_cv(train_data2, v = 10, strata = amphet_use)
boost <- boost_tree(
trees = tune(),
learn_rate = tune(),
tree_depth = tune(),
loss_reduction = tune(),
min_n = tune(),
sample_size = tune(),
) %>%
set_mode("classification") %>%
set_engine("xgboost", objective = "binary:logistic")
workflow_boost <- workflow() %>%
add_recipe(recipe) %>%
add_model(boost)
fit_boost  <- tune_grid(
workflow_boost,
cv,
metrics = metric_set(accuracy)
)
fit_boost  <- tune_grid(
workflow_boost,
cv,
metrics = metric_set(roc_auc)
)
final_fit <- workflow_boost %>%
update_model(boost_final) %>%
fit(data = train_data2)
preds <- predict(final_fit, new_data = test_data2, type = "class") %>%
bind_cols(test_data2 %>% select(amphet_use))
metrics(preds, truth = amphet_use, estimate = .pred_class)
recipe <- recipe(amphet_use ~ ., data = train_data2)
cv <- vfold_cv(train_data2, v = 10, strata = amphet_use)
boost <- boost_tree(
trees = tune(),
learn_rate = tune(),
tree_depth = tune(),
loss_reduction = tune(),
min_n = tune(),
sample_size = tune(),
mtry = tune()
) %>%
set_mode("classification") %>%
set_engine("xgboost", objective = "binary:logistic")
workflow_boost <- workflow() %>%
add_recipe(recipe) %>%
add_model(boost)
fit_boost  <- tune_grid(
workflow_boost,
cv,
metrics = metric_set(roc_auc)
)
boost_best <- fit_boost %>%
select_best() # best tuning parameters
boost_final <- finalize_model(
boost,
boost_best
)
boost_final
final_fit <- workflow_boost %>%
update_model(boost_final) %>%
fit(data = train_data2)
preds <- predict(final_fit, new_data = test_data2, type = "class") %>%
bind_cols(test_data2 %>% select(amphet_use))
metrics(preds, truth = amphet_use, estimate = .pred_class)
# Predict probabilities on training data
pred_class_train <- predict(final_fit, new_data = train_data2, type = "class")$.pred_class
train_acc <- mean(pred_class_train == train_data2$amphet_use)
cat("XGBoost Training Accuracy:", round(train_acc, 3), "\n")
# Predict probabilities on test data
pred_class_test <- predict(final_fit, new_data = test_data2, type = "class")$.pred_class
test_acc <- mean(pred_class_test == test_data2$amphet_use)
cat("XGBoost Test Accuracy:", round(test_acc, 3), "\n")
nrow(intersect(train_data2, test_data2))  # should be 0
pred_train <- predict(final_fit, train_data2) %>% pull(.pred_class)
pred_test  <- predict(final_fit, test_data2)  %>% pull(.pred_class)
train_acc <- mean(pred_train == train_data2$amphet_use)
test_acc  <- mean(pred_test  == test_data2$amphet_use)
cat("XGB Accuracy (Train):", round(train_acc,3), "\n")
cat("XGB Accuracy (Test) :", round(test_acc,3), "\n")
table(preds$.pred_class)
table(train_data2$amphet_use)
table(test_data2$amphet_use)
data <- read.csv("data/drug_consumption.csv")
#data <- read.csv("~/Desktop/ST310 Machine Learning/project/drug_consumption.csv")
# Keep only selected drug variables along with the original features
selected_vars <- c("age", "gender", "education", "country", "ethnicity", "nscore",
"escore", "oscore","ascore", "cscore", "impuslive", "ss",
"alcohol", "choc", "cannabis", "caff", "nicotine", "amphet")
drug_data <-  data[, selected_vars]
# Rename 'impuslive' to 'impulsive'
colnames(drug_data)[colnames(drug_data) == "impuslive"] <- "impulsive"
head(drug_data)
table(drug_data$amphet)
drug_data2 <- drug_data
drug_vars <- c("alcohol", "choc", "cannabis", "caff", "nicotine", "amphet")
# Convert all to ordered numeric levels
for (var in drug_vars) {
drug_data2[[paste0(var, "_level")]] <- as.numeric(factor(drug_data2[[var]],
levels = c("CL0", "CL1", "CL2", "CL3", "CL4", "CL5", "CL6"))) - 1
}
# Remove the original categorical drug columns
drug_data2 <- drug_data2[ , !(names(drug_data2) %in% drug_vars)]
# Create binary target: 0 = no use, 1 = used
drug_data2$amphet_use <- ifelse(drug_data2$amphet_level > 0, 1, 0)
drug_data2$amphet_use <- as.factor(drug_data2$amphet_use)
# Remove amphet_level
drug_data2$amphet_level <- NULL
head(drug_data2)
set.seed(38036)
train_indices <- createDataPartition(drug_data2$amphet_use, p = 0.8, list = FALSE)
train_data2 <- drug_data2[train_indices, ]
test_data2  <- drug_data2[-train_indices, ]
head(train_data2)
table(train_data2$amphet_use)
recipe <- recipe(amphet_use ~ ., data = train_data2)
cv <- vfold_cv(train_data2, v = 10, strata = amphet_use)
boost <- boost_tree(
trees = tune(),
learn_rate = tune(),
tree_depth = tune(),
loss_reduction = tune(),
min_n = tune(),
sample_size = tune(),
mtry = tune()
) %>%
set_mode("classification") %>%
set_engine("xgboost", objective = "binary:logistic")
workflow_boost <- workflow() %>%
add_recipe(recipe) %>%
add_model(boost)
fit_boost  <- tune_grid(
workflow_boost,
cv,
metrics = metric_set(roc_auc)
)
boost_best <- fit_boost %>%
select_best() # best tuning parameters
boost_final <- finalize_model(
boost,
boost_best
)
boost_final
final_fit <- workflow_boost %>%
update_model(boost_final) %>%
fit(data = train_data2)
preds <- predict(final_fit, new_data = test_data2, type = "class") %>%
bind_cols(test_data2 %>% select(amphet_use))
metrics(preds, truth = amphet_use, estimate = .pred_class)
pred_train <- predict(final_fit, train_data2) %>% pull(.pred_class)
pred_test  <- predict(final_fit, test_data2)  %>% pull(.pred_class)
train_acc <- mean(pred_train == train_data2$amphet_use)
test_acc  <- mean(pred_test  == test_data2$amphet_use)
cat("XGB Accuracy (Train):", round(train_acc,3), "\n")
cat("XGB Accuracy (Test) :", round(test_acc,3), "\n")
pred_train <- predict(final_fit, train_data2) %>% pull(.pred_class)
pred_test  <- predict(final_fit, test_data2)  %>% pull(.pred_class)
xgb_train_acc <- mean(pred_train == train_data2$amphet_use)
xgb_test_acc  <- mean(pred_test  == test_data2$amphet_use)
cat("XGB Accuracy (Train):", round(xgb_train_acc,3), "\n")
cat("XGB Accuracy (Test) :", round(xgb_test_acc,3), "\n")
pred_train <- predict(final_fit, train_data2) %>% pull(.pred_class)
pred_test  <- predict(final_fit, test_data2)  %>% pull(.pred_class)
xgb_train_acc <- mean(pred_train == train_data2$amphet_use)
xgb_test_acc  <- mean(pred_test  == test_data2$amphet_use)
cat("XGB Accuracy (Train):", round(xgb_train_acc,3), "\n")
cat("XGB Accuracy (Test) :", round(xgb_test_acc,3), "\n")
conf_mat_test <- conf_mat(
tibble(truth = test_data2$amphet_use, prediction = pred_test),
truth = truth,
estimate = prediction
)
conf_mat_test
roc_data <- roc_curve(preds, truth = amphet_use, .pred_1)
pred_probs <- predict(final_fit, new_data = test_data2, type = "prob") %>%
bind_cols(test_data2 %>% select(amphet_use))
roc_data <- roc_curve(pred_probs, truth = amphet_use, .pred_1)
autoplot(roc_data)
pred_probs <- predict(final_fit, new_data = test_data2, type = "prob") %>%
bind_cols(test_data2 %>% select(amphet_use))
roc_data <- roc_curve(pred_probs, truth = amphet_use, .pred_0)
autoplot(roc_data)
pred_probs <- predict(final_fit, new_data = test_data2, type = "prob") %>%
bind_cols(test_data2 %>% select(amphet_use))
roc_data <- roc_curve(pred_probs, truth = amphet_use, .pred_0)
autoplot(roc_data)
xgb_auc <- roc_auc(pred_probs, truth = amphet_use, .pred_0)
xgb_auc
