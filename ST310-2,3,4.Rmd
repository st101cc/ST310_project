---
title: "Hero"
output: html_document
date: "2025-04-09"
---

---
title: "ST310 Project"
author: "Canyin Yu"
date: "2025-04-06"
output: html_document
---

```{r setup1, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
```

```{r}
data <- read.csv("drug_consumption.csv")

# Keep only selected drug variables along with the original features
selected_vars <- c("age", "gender", "education", "country", "ethnicity", "nscore", 
                   "escore", "oscore","ascore", "cscore", "impuslive", "ss", 
                   "alcohol", "choc", "cannabis", "caff", "nicotine", "heroin")
drug_data <-  data[, selected_vars]
# Rename 'impuslive' to 'impulsive'
colnames(drug_data)[colnames(drug_data) == "impuslive"] <- "impulsive"

head(drug_data)
table(drug_data$heroin)
```
```{r}
str(drug_data)
```

import libraries:
```{r}
library(caret)
library(tidymodels)
library(ggplot2)
library(dplyr)
library(yardstick)
```
### Custom Gradient Descent Model (Softmax Regression)
2. At least one model should be fit using your own implementation of gradient descent. The only restrictions on this model are that the gradient of the loss function should not be a constant. You are free to use a simple model and a simple loss function to make the derivation and computation manageable.

To implement a model using own gradient descent, I developed a multiclass softmax regression model to predict heroin comsumption levels (CL0 to CL6). This model uses the cross-entropy loss function, and the gradient was derived with respect to model weights. The implementation involved one-hot encoding the outcome variable and using a custom softmax function to compute class probabilities. I manually performed gradient descent over 500 iterations with a learning rate of 0.05. This model achieved an accuracy of 85.8% on the testing set.

Convert all drug variables into ordered factor (0–6):
```{r}
drug_data2 <- drug_data
drug_vars <- c("alcohol", "choc", "cannabis", "caff", "nicotine", "heroin")

# Convert all to ordered numeric levels
for (var in drug_vars) {
  drug_data2[[paste0(var, "_level")]] <- as.numeric(factor(drug_data2[[var]], 
                                                           levels = c("CL0", "CL1", "CL2", "CL3", "CL4", "CL5", "CL6"))) - 1
}
```

Split data into training and testing set:
```{r}
set.seed(123)
train_indices <- createDataPartition(drug_data2$heroin, p = 0.8, list = FALSE)

train_data2 <- drug_data2[train_indices, ]
test_data2  <- drug_data2[-train_indices, ]
```

Define functions:
```{r}
# Softmax function:
softmax <- function(z) {
  exp_z <- exp(z - apply(z, 1, max))
  exp_z / rowSums(exp_z)
}

# One-hot encode the outcome variable - heroin: 
one_hot <- function(y, num_classes) {
  Y <- matrix(0, nrow = length(y), ncol = num_classes)
  Y[cbind(1:length(y), y + 1)] <- 1
  return(Y)
}

# Cross-entropy loss:
compute_loss <- function(X, Y, weights) {
  logits <- X %*% weights
  probs <- softmax(logits)
  -sum(Y * log(probs + 1e-10)) / nrow(X)
}

# Gradient of loss:
compute_gradient <- function(X, Y, weights) {
  logits <- X %*% weights
  probs <- softmax(logits)
  grad <- t(X) %*% (probs - Y) / nrow(X)
  return(grad)
}
```

Define Gradient Descent function:
```{r}
gradient_descent_softmax <- function(X, y, num_classes, lr = 0.05, n_iter = 500) {
  num_samples <- nrow(X)
  num_features <- ncol(X)
  Y <- one_hot(y, num_classes)
  weights <- matrix(0, nrow = num_features, ncol = num_classes)

  for (i in 1:n_iter) {
    grad <- compute_gradient(X, Y, weights)
    weights <- weights - lr * grad
    if (i %% 50 == 0) {
      loss <- compute_loss(X, Y, weights)
      cat("Iteration:", i, "Loss:", loss, "\n")
    }
  }
  return(weights)
}
```

Prepare Data & Train the Model:
```{r}
# Select all numeric variables 
predictor_vars <- c("age", "gender", "education", "country", "ethnicity",
                    "nscore", "escore", "oscore", "ascore", "cscore",
                    "impulsive", "ss", "alcohol_level", "choc_level", 
                    "cannabis_level", "caff_level", "nicotine_level")

# Design matrix for training
X_train <- as.matrix(train_data2[, predictor_vars])
X_train <- scale(X_train)
X_train <- cbind(1, X_train)

y_train <- train_data2$heroin_level
num_classes <- length(unique(y_train))

# Train the model
weights_trained <- gradient_descent_softmax(X_train, y_train, num_classes)
```

Evaluate on Test and Train Data:
```{r}
# ==== Test Data ====
X_test <- as.matrix(test_data2[, predictor_vars])
X_test <- scale(X_test)
X_test <- cbind(1, X_test)

y_test <- test_data2$heroin_level

logits_test <- X_test %*% weights_trained
probs_test <- softmax(logits_test)
predictions_test <- max.col(probs_test) - 1 

test_accuracy <- mean(predictions_test == y_test)
cat("Gradient Decent Accuracy (Test Set):", round(test_accuracy, 3), "\n")

# ==== Train Data ====
X_train <- as.matrix(train_data2[, predictor_vars])
X_train <- scale(X_train)
X_train <- cbind(1, X_train)

y_train <- train_data2$heroin_level

logits_train <- X_train %*% weights_trained
probs_train <- softmax(logits_train)
predictions_train <- max.col(probs_train) - 1 

train_accuracy <- mean(predictions_train == y_train)
cat("Gradient Decent Accuracy (Train Set):", round(train_accuracy, 3), "\n")
```

```{r}
# Create a confusion matrix as a dataframe
conf_mat_df <- table(Predicted = predictions, Actual = y_test) %>%
  as.data.frame()

# Convert levels to factors
conf_mat_df$Predicted <- factor(conf_mat_df$Predicted, levels = 0:6, labels = paste0("CL", 0:6))
conf_mat_df$Actual <- factor(conf_mat_df$Actual, levels = 0:6, labels = paste0("CL", 0:6))

# Heatmap plot
ggplot(conf_mat_df, aes(x = Actual, y = Predicted, fill = Freq)) +
  geom_tile(color = "white") +
  geom_text(aes(label = Freq), size = 4) +
  scale_fill_gradient(low = "white", high = "steelblue") +
  labs(title = "Confusion Matrix: Softmax Regression (Gradient Descent)",
       x = "Heroin Comsumption Level", y = "Predicted Level", fill = "Count") +
  theme_minimal()
```
### Interpretable Model: Decision Tree

3. At least one non-baseline model must be (relatively) interpretable. For this model you should write a brief sub-section including your interpretation of the results. You could compare to a baseline model on both predictive accuracy and (in)consistency of interpretations.

To ensure interpretability, a Decision Tree model was implemented to classify individuals into categories of heroin use (CL0 to CL6) with CL0 (Never Used) being the most common outcome. The tree was trained using demographic variables (e.g. age, gender, education, country, ethnicity), personality traits (e.g. NEO-FFI-R scores, impulsiveness, and sensation-seeking), and usage patterns of other drugs (e.g. alcohol, nicotine, cannabis).

The **root split** is made on the variable `country` at the threshold of −1.3. This suggests that geographical location is the most important predictor of heroin usage levels. For instance, individuals living in countries with a `country` score above –1.3 (e.g. UK and Canada) are more likely to be classified as **CL0** (never used heroin). 

The left subtree reveals that lower levels of **nicotine** and **cannabis** consumption further predict non-use (CL0), especially when accompanied by lower **alcohol** usage and higher **conscientiousness (cscore)**. On the other hand, lower **agreeableness** (ascore < −2.2) and higher **sensation seeing** (ss >= 1) increase the likelihood of being categorized in a higher comsuption level. 

**Age** is also a significant predictor. For example, younger participants (age < 0.2, corresponding to the 18–34 yeasrs old range) are more likely to be routed toward higher-risk heroin use categories (CL3, CL4), whereas older individuals tend to stay in CL0.

Moreover, **education** emerges as a key predictor in the deeper levels of the tree. Individuals with lower **education** (education < −1.5, e.g., individuals left school at or before age 16 ) tend to classify into non-CL0 categories, such as CL1 or even CL4.

The **variable importance** also supports `country`, `age`, `nicotine_CL6`, and `cannabis_CL6` were the most influential variables, indicating that both geographical and behavioral patterns play significant roles in heroin consumption prediction. Among personality traits, **neuroticism (nscore)**, **agreeableness (ascore)**, and **impulsiveness** contributed moderately.

Compared to the baseline model, 

In conclusion, the Decision Tree serves as a relatively interpretable model, clearly outlining how specific behavioral and demographic factors influence heroin consumption classification.

```{r}
# Factor all drug variables:
drug_vars <- c("alcohol", "choc", "cannabis", "caff", "nicotine", "heroin")

drug_data[drug_vars] <- lapply(drug_data[drug_vars], function(x) {
  factor(x, levels = paste0("CL", 0:6))
})

# Split data into training and testing set:
set.seed(123)
train_indices <- createDataPartition(drug_data$heroin, p = 0.8, list = FALSE)

train_data <- drug_data[train_indices, ]
test_data  <- drug_data[-train_indices, ]
```

#### Build Decision Tree:
```{r}
# Data Recipe
data_recipe <- recipe(heroin ~ ., data = train_data) %>%
  step_dummy(all_nominal_predictors(), -all_outcomes()) %>%
  step_normalize(all_numeric_predictors())

# Define decision tree model
tree_spec <- decision_tree(
  cost_complexity = 0.001,  
  min_n = 10,               
  tree_depth = 5           
) %>%
  set_engine("rpart", model = TRUE) %>%
  set_mode("classification")

# Create workflow
tree_workflow <- workflow() %>%
  add_recipe(data_recipe) %>%
  add_model(tree_spec)

# Fit model to training data
tree_fit <- fit(tree_workflow, data = train_data)

# Predict on test data and get test accuracy
tree_preds_test <- predict(tree_fit, new_data = test_data) %>%
  bind_cols(test_data)

# Get training accuracy
tree_preds_train <- predict(tree_fit, new_data = train_data) %>%
  bind_cols(train_data)

# Define the metric set
class_metrics <- metric_set(yardstick::accuracy)

# Evaluate test accuracy
test_accuracy <- tree_preds_test %>%
  class_metrics(truth = heroin, estimate = .pred_class)

# Evaluate train accuracy
train_accuracy <- tree_preds_train %>%
  class_metrics(truth = heroin, estimate = .pred_class)

# Print accuracies
cat("Decision Tree Accuracy (Test Set):", round(test_accuracy$.estimate, 3), "\n")
cat("Decision Tree Accuracy (Train Set):", round(train_accuracy$.estimate, 3), "\n")
```

```{r}
library(rpart.plot)

fitted_tree <- extract_fit_engine(tree_fit)
rpart.plot(fitted_tree, type = 4, extra = 104)
#summary(extract_fit_engine(tree_fit))
```
```{r}
fitted_tree <- extract_fit_engine(tree_fit)
fitted_tree$variable.importance
```
```{r}
library(vip)

tree_fit %>%
  extract_fit_parsnip() %>%
  vip()
```


```{r}
# Convert drugs to numeric 0–6 levels
drug_vars <- c("alcohol", "choc", "cannabis", "caff", "nicotine", "heroin")
for (var in drug_vars) {
  drug_data[[paste0(var, "_level")]] <- as.numeric(factor(drug_data[[var]], 
                                                           levels = paste0("CL", 0:6))) - 1
}

drug_data2 <- drug_data
predictor_vars <- c("age", "gender", "education", "country", "ethnicity",
                    "nscore", "escore", "oscore", "ascore", "cscore",
                    "impulsive", "ss", "alcohol_level", "choc_level", 
                    "cannabis_level", "caff_level", "nicotine_level")
```

```{r}
# Split Data
set.seed(888)
train_indices <- createDataPartition(drug_data2$heroin_level, p = 0.8, list = FALSE)
train_data2 <- drug_data2[train_indices, ]
test_data2  <- drug_data2[-train_indices, ]
```
### Build Random Forest

```{r}
set.seed(2025)
rf_model <- train(
  x = train_data2[, predictor_vars],
  y = as.factor(train_data2$heroin_level),
  method = "rf",
  trControl = trainControl(method = "cv", number = 5),
  tuneGrid = expand.grid(mtry = c(2, 3, 4))
)
```

```{r}
# Evaluate Model
pred_train <- predict(rf_model, newdata = train_data2[, predictor_vars])
pred_test <- predict(rf_model, newdata = test_data2[, predictor_vars])

train_acc <- mean(pred_train == train_data2$heroin_level)
test_acc <- mean(pred_test == test_data2$heroin_level)

cat("Random Forest Accuracy (Train Set):", round(train_acc, 3), "\n")
cat("Random Forest Accuracy (Test Set):", round(test_acc, 3), "\n")
```

```{r}
var_imp <- varImp(rf_model)
print(var_imp)
```
```{r}
plot(var_imp, top = 20)
```
### High Dimensional Model: Random Forest

4. At least one model must be (relatively) high-dimensional. If your dataset has many predictors, and the number of observations is not much larger, then for example you could fit a penalized regression model using all the predictors. If your dataset does not have many predictors you could consider models that include non-linear transformations, interaction terms, and/or local smoothing to increase the effective degrees of freedom.

The high-dimensional model chosen is Random Forest. It achieved a test set accuracy of 84.6%, demonstrating strong predictive performance for the multi-class task of heroin consumption level classification. The model selected and leveraged a wide range of features, with personality traits emerging as the most influential.

According to the variable importance results, the NEO-FFI-R personality dimensions—particularly agreeableness (ascore), neuroticism (nscore), conscientiousness (cscore), openness (oscore), and extraversion (escore)—were the top predictors. This suggests that personality plays a central role in differentiating heroin use levels.

In addition, behavioral traits like sensation seeking (ss), impulsiveness, and education level were also significant. Among drug-use features, cannabis consumption and alcohol use were the most predictive.

These results highlight the complex interplay between psychological traits, substance use behaviors, and demographic factors in understanding patterns of heroin consumption.
