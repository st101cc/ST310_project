---
title: "ST310 Project"
date: "2025-04-11"
output: html_document
---

```{r setup1, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
rm(list = ls())
```

# Load libraries:
```{r}
library(caret)
library(ggplot2)
library(xgboost)
library(caret)
library(Matrix)
library(ROCR)
library(tidymodels)
library(dplyr)
library(yardstick)
library(smotefamily)
library(pROC)
library(lightgbm)
library(ROSE)
```

# Load data:
```{r}
data <- read.csv("data/drug_consumption.csv")
#data <- read.csv("~/Desktop/ST310 Machine Learning/project/drug_consumption.csv")

# Keep only selected drug variables along with the original features
selected_vars <- c("age", "gender", "education", "country", "ethnicity", "nscore", 
                   "escore", "oscore","ascore", "cscore", "impuslive", "ss", 
                   "alcohol", "choc", "cannabis", "caff", "nicotine", "amphet")
drug_data <-  data[, selected_vars]

# Rename 'impuslive' to 'impulsive'
colnames(drug_data)[colnames(drug_data) == "impuslive"] <- "impulsive"

head(drug_data)
table(drug_data$amphet)
```

## Check data structure:
```{r}
str(drug_data)
```

# Initial data manipulation

## Convert all drug variables into ordered factor (0–6):
```{r}
drug_data2 <- drug_data
drug_vars <- c("alcohol", "choc", "cannabis", "caff", "nicotine", "amphet")

# Convert all to ordered numeric levels
for (var in drug_vars) {
  drug_data2[[paste0(var, "_level")]] <- as.numeric(factor(drug_data2[[var]], 
                                                           levels = c("CL0", "CL1", "CL2", "CL3", "CL4", "CL5", "CL6"))) - 1
}

# Remove the original categorical drug columns
drug_data2 <- drug_data2[ , !(names(drug_data2) %in% drug_vars)]
```

## Convert amphet to binary factor: 
- 0 = no use (CL0)
- 1 = used (CL1 to CL6)
```{r}
# Create binary target: 0 = no use, 1 = used
drug_data2$amphet_use <- ifelse(drug_data2$amphet_level > 0, 1, 0)
drug_data2$amphet_use <- as.factor(drug_data2$amphet_use)

# Remove amphet_level
drug_data2$amphet_level <- NULL

head(drug_data2)
```

## Check data structure:
```{r}
str(drug_data2)
```

## Split data into training and testing set:
```{r}
set.seed(38036)

train_indices <- createDataPartition(drug_data2$amphet_use, p = 0.8, list = FALSE)

train_data2 <- drug_data2[train_indices, ]
test_data2  <- drug_data2[-train_indices, ]

head(train_data2)
table(train_data2$amphet_use)
```

# Baseline model: logistic regression

## Logistic regression model:
```{r}
base_model <- glm(amphet_use ~ . , data = train_data2, family = binomial)
summary(base_model)
```

The result from logistic regression indicate that the most significant predictors of amphet use are age, gender, ss (sensation seeking), alcohol_level, cannabis level, caff_level, and nicotine_level. This finding is consistent with behavioral expectations: individuals who are older, male sex, and exhibit higher levels of sensation seeking are more inclined to experiment with substances like Amphetamine Additionally, individuals who consume less alcohol and those who use more cannabis, caffeine, and nicotine are more likely use Amphetamine

## Compute accuracy:
```{r}
# Predict on test data
pred_probs_test <- predict(base_model, newdata = test_data2, type = "response")
pred_class_test <- ifelse(pred_probs_test > 0.5, 1, 0)
base_test_accuracy <- mean(pred_class_test == test_data2$amphet_use)
cat("Logistic Regression Test Accuracy:", round(base_test_accuracy, 3), "\n")

# Predict on training data
pred_probs_train <- predict(base_model, newdata = train_data2, type = "response")
pred_class_train <- ifelse(pred_probs_train > 0.5, 1, 0)
base_train_accuracy <- mean(pred_class_train == train_data2$amphet_use)
cat("Logistic Regression Training Accuracy:", round(base_train_accuracy, 3), "\n")
```

## Confusion matrix:
```{r}
# Convert to factors for confusionMatrix function
pred_class <- factor(pred_class_test, levels = c(0, 1))
actual_class <- factor(test_data2$amphet_use, levels = c(0, 1))

# Confusion matrix
cm <- confusionMatrix(pred_class, actual_class, positive = "1")
print(cm)

# Plot confusion matrix
cm_table <- as.data.frame(cm$table)
colnames(cm_table) <- c("Predicted", "Actual", "Freq")

ggplot(data = cm_table, aes(x = Actual, y = Predicted, fill = Freq)) +
  geom_tile(color = "white") +
  geom_text(aes(label = Freq), size = 6) +
  scale_fill_gradient(low = "white", high = "steelblue") +
  labs(title = "Logistic Regression Confusion Matrix", x = "Actual", y = "Predicted") +
  theme_minimal()
```

## Compute ROC:
```{r}
true_labels <- as.numeric(as.character(test_data2$amphet_use))
roc_obj <- roc(response = true_labels, predictor = pred_probs_test)

# Print auc
log_auc <- auc(roc_obj)
print(log_auc)

# Plot ROC curve
plot(roc_obj, main = "Logistic Regression ROC Curve", col = "blue")
```
The model's AUC is 0.80, indicating that there is a moderate 80% chance that the model ranks a random positive case (Amphetamine user) higher than a random negative case (non-user). Overall, the baseline model is performing relatively well with a test accuracy of 70%. 

# Custom Gradient Descent Model (Sigmoid Function)
2. At least one model should be fit using your own implementation of gradient descent. The only restrictions on this model are that the gradient of the loss function should not be a constant. You are free to use a simple model and a simple loss function to make the derivation and computation manageable.

To implement a model using own gradient descent, I developed a multiclass softmax regression model to predict heroin comsumption levels (CL0 to CL6). This model uses the cross-entropy loss function, and the gradient was derived with respect to model weights. The implementation involved one-hot encoding the outcome variable and using a custom softmax function to compute class probabilities. I manually performed gradient descent over 500 iterations with a learning rate of 0.05. This model achieved an accuracy of 85.8% on the testing set.

## Define functions:
```{r}
# Sigmoid Function:
sigmoid <- function(z) {
  1 / (1 + exp(-z))
}

# Binary Cross-Entropy Loss:
compute_loss_bin <- function(X, y, weights) {
  z <- X %*% weights
  p <- sigmoid(z)
  # Adding a small value (1e-10) to avoid log(0)
  loss <- -mean(y * log(p + 1e-10) + (1 - y) * log(1 - p + 1e-10))
  return(loss)
}

# Gradient of Loss:
compute_gradient_bin <- function(X, y, weights) {
  z <- X %*% weights
  p <- sigmoid(z)
  grad <- t(X) %*% (p - y) / nrow(X)
  return(grad)
}


# Define Gradient Descent function:
gradient_descent <- function(X, y, lr = 0.05, n_iter = 500) {
  num_features <- ncol(X)
  weights <- matrix(0, nrow = num_features, ncol = 1)
  
  for (i in 1:n_iter) {
    grad <- compute_gradient_bin(X, y, weights)
    weights <- weights - lr * grad
    if (i %% 50 == 0) {
      loss <- compute_loss_bin(X, y, weights)
      cat("Iteration:", i, "Loss:", loss, "\n")
    }
  }
  return(weights)
}
```

## Prepare Data & Train the Model:
```{r}
set.seed(38036)
# Removing the target variable
X_train <- as.matrix(train_data2[, -ncol(train_data2)])

X_train <- scale(X_train)
X_train <- cbind(1, X_train)

y_train <- as.numeric(as.character(train_data2[, ncol(train_data2)]))
y_train <- matrix(y_train, ncol = 1)

weights_trained <- gradient_descent(X_train, y_train, lr = 0.05, n_iter = 500)
```

## Evaluate on Test and Train Data:
```{r}
# Test Data
X_test <- as.matrix(test_data2[, -ncol(test_data2)])
X_test <- scale(X_test)
X_test <- cbind(1, X_test)

y_test <- as.numeric(as.character(test_data2[, ncol(test_data2)]))
y_test <- matrix(y_test, ncol = 1)

# Predict probabilities using sigmoid
predict_prob_test <- sigmoid(X_test %*% weights_trained)
predict_class_test <- ifelse(predict_prob_test >= 0.5, 1, 0)

sigmoid_test_accuracy <- mean(predict_class_test == y_test)
cat("Gradient Descent Accuracy (Test Set):", round(sigmoid_test_accuracy, 3), "\n")

# Train Data
# Predict probabilities
predict_prob_train <- sigmoid(X_train %*% weights_trained)
predict_class_train <- ifelse(predict_prob_train >= 0.5, 1, 0)

sigmoid_train_accuracy <- mean(predict_class_train == y_train)
cat("Gradient Descent Accuracy (Train Set):", round(sigmoid_train_accuracy, 3), "\n")
```

## Confusion Martrix for Gradient Descent (Test Set):
```{r}
pred_test <- factor(predict_class_test, levels = c(0, 1))
actual_test <- factor(test_data2$amphet_use, levels = c(0, 1))

confusionMatrix(pred_test, actual_test)
```

## Compute ROC:
```{r}
true_labels <- as.numeric(as.character(test_data2$amphet_use))
roc_sigmoid <- roc(response = true_labels, predictor = predict_prob_test)

# Print auc
sigmoid_auc <- auc(roc_sigmoid)
print(sigmoid_auc)

# Plot ROC curve
plot(roc_sigmoid, main = "ROC Curve - Gradient Descent (Sigmoid)", col = "blue")
```

# Interpretable Model: Lasso Regression
3. At least one non-baseline model must be (relatively) interpretable. For this model you should write a brief sub-section including your interpretation of the results. You could compare to a baseline model on both predictive accuracy and (in)consistency of interpretations.


Compared to the baseline model, 

In conclusion, the Decision Tree serves as a relatively interpretable model, clearly outlining how specific behavioral and demographic factors influence heroin consumption classification.

## Lasso regression model:
```{r}
train_data2$amphet_use <- as.factor(train_data2$amphet_use)
test_data2$amphet_use <- as.factor(test_data2$amphet_use)

# Build Lasso Model
lasso_recipe <- recipe(amphet_use ~ ., data = train_data2) %>%
  step_normalize(all_predictors()) 

lasso_spec <- logistic_reg(
  mode = "classification",
  penalty = tune(),       
  mixture = 1 
) %>%
  set_engine("glmnet")

set.seed(38036)
folds <- vfold_cv(train_data2, v = 10)

# Workflow
lasso_workflow <- workflow() %>%
  add_recipe(lasso_recipe) %>%
  add_model(lasso_spec)

lasso_grid <- grid_regular(penalty(), levels = 30)

tuned_lasso <- tune_grid(
  lasso_workflow,
  resamples = folds,
  grid = lasso_grid,
  metrics = metric_set(accuracy, roc_auc)
)

# Best lambda
best_lasso <- select_best(tuned_lasso, metric = "accuracy")
print(best_lasso)

final_lasso <- finalize_workflow(lasso_workflow, best_lasso)

lasso_fit <- fit(final_lasso, data = train_data2)
```

## Evaluate on Test and Train Data:
```{r}
# Test Data:
pred_test <- predict(lasso_fit, new_data = test_data2, type = "class") %>%
  pull(.pred_class)

# Predict probabilities 
probs_test <- predict(lasso_fit, new_data = test_data2, type = "prob")

# Test Accuracy
lasso_test_acc <- mean(pred_test == test_data2$amphet_use)
cat("Lasso Regression Accuracy (Test Set):", round(lasso_test_acc, 3), "\n")

# Train Data:
pred_train <- predict(lasso_fit, new_data = train_data2, type = "class") %>%
  pull(.pred_class)

# Train Accuracy
lasso_train_acc <- mean(pred_train == train_data2$amphet_use)
cat("Lasso Regression Accuracy (Train Set):", round(lasso_train_acc, 3), "\n")
```

```{r}
lasso_coef <- lasso_fit %>%
  extract_fit_parsnip() %>%
  tidy() %>%
  filter(estimate != 0) %>%
  arrange(desc(abs(estimate)))

print(lasso_coef)
```

## Confusion Matrix for Lasso (Test Set):
```{r}
confusionMatrix(pred_test, test_data2$amphet_use)
```

## Compute ROC & Compare with Baseline Logistic Model
```{r}
true_labels <- test_data2$amphet_use
pred_probs_lasso <- probs_test %>% pull(.pred_1)

roc_lasso <- roc(response = true_labels, predictor = pred_probs_lasso)
lasso_auc <- auc(roc_lasso)
print(lasso_auc)

# Baseline Logistic's ROC
plot(roc_obj, main = "ROC Curve Comparison: Logistic vs Lasso",
     col = "blue", lwd = 2)

# Lasso's ROC
lines(roc_lasso, col = "red", lwd = 2, lty = 2)

legend("bottomright",
       legend = c(paste0("Logistic (AUC = ", round(log_auc, 4), ")"),
                  paste0("Lasso (AUC = ", round(auc_lasso, 4), ")")),
       col = c("blue", "red"),
       lty = c(1, 2),
       lwd = 2)
```

# High Dimensional Model: Random Forest
4. At least one model must be (relatively) high-dimensional. If your dataset has many predictors, and the number of observations is not much larger, then for example you could fit a penalized regression model using all the predictors. If your dataset does not have many predictors you could consider models that include non-linear transformations, interaction terms, and/or local smoothing to increase the effective degrees of freedom.

The high-dimensional model implemented was a Random Forest classifier to predict binary heroin use (use vs. no use). To address class imbalance in the training data, oversampling was applied. The model achieved a test set accuracy of 84.4%.

The most important predictors, as indicated by variable importance, are: Cannabis use, country, and neuroticism (nscore) – the top three signals for predicting heroin use. Other key variables included personality traits such as openness (oscore), agreeableness (ascore), conscientiousness (cscore), and extraversion (escore), as well as sensation seeking, nicotine use, and impulsiveness. These results underscore a complex relationship between personality dimensions, other substance use patterns, and geographic context in shaping the likelihood of heroin use.

## Build Random Forest:
```{r}
set.seed(38036)
rf_model <- train(
  x = train_data3[, -ncol(train_data3)],
  y = train_data3$heroin_use,
  method = "rf",
  trControl = trainControl(method = "cv", number = 5),
  tuneGrid = expand.grid(mtry = c(2, 3, 4))
)
```

```{r}
# Predict on oversampled model
pred_train <- predict(rf_model, newdata = train_data3[, -ncol(train_data3)])
pred_test <- predict(rf_model, newdata = test_data2[, -ncol(test_data2)])

train_acc <- mean(pred_train == train_data3$heroin_use)
test_acc <- mean(pred_test == test_data2$heroin_use)

cat("Random Forest Accuracy (Train Set):", round(train_acc, 3), "\n")
cat("Random Forest Accuracy (Test Set):", round(test_acc, 3), "\n")
```

```{r}
confusionMatrix(pred_test, test_data2$heroin_use)
```

```{r}
var_imp <- varImp(rf_model)
print(var_imp)
```

```{r}
plot(var_imp, top = 20)
```

# Predictive model: XGB 

## Prepare data:
```{r}
train_data2$amphet_use <- factor(train_data2$amphet_use, levels = c(0, 1), labels = c("no", "yes"))
test_data2$amphet_use  <- factor(test_data2$amphet_use,  levels = c(0, 1), labels = c("no", "yes"))

X_train <- train_data2[, setdiff(names(train_data2), "amphet_use")]
X_test <- test_data2[, setdiff(names(test_data2), "amphet_use")]
y_train <- train_data2$amphet_use
y_test  <- test_data2$amphet_use
```

## Define train control:
```{r}
ctrl <- trainControl(
  method = "cv",      
  number = 5,
  verboseIter = TRUE,
  classProbs = TRUE,   
  summaryFunction = twoClassSummary
)
```

## Parameter grid:
```{r}
grid <- expand.grid(
  nrounds = c(50, 100),
  max_depth = c(3, 6, 9),
  eta = c(0.01, 0.1, 0.3),
  gamma = 0,
  colsample_bytree = c(0.7, 1),
  min_child_weight = 1,
  subsample = c(0.7, 1)
)
```

## XGB model:
```{r}
set.seed(38036)

xgb_caret <- train(
  x = X_train,
  y = y_train,
  method = "xgbTree",
  trControl = ctrl,
  tuneGrid = grid,
  metric = "ROC"
)
```

```{r}
print(xgb_caret$bestTune)
plot(xgb_caret)
```

## Accuracy:
```{r}
# Predict probabilities on test data
xgb_pred_probs_test <- predict(xgb_caret, newdata = test_data2, type = "prob")[, "yes"]
xgb_pred_class_test <- ifelse(xgb_pred_probs_test > 0.5, "yes", "no")
xgb_test_accuracy <- mean(xgb_pred_class_test == test_data2$amphet_use)
cat("XGBoost Test Accuracy:", round(xgb_test_accuracy, 3), "\n")

# Predict probabilities on training data
xgb_pred_probs_train <- predict(xgb_caret, newdata = train_data2, type = "prob")[, "yes"]
xgb_pred_class_train <- ifelse(xgb_pred_probs_train > 0.5, "yes", "no")
xgb_train_accuracy <- mean(xgb_pred_class_train == train_data2$amphet_use)
cat("XGBoost Training Accuracy:", round(xgb_train_accuracy, 3), "\n")
```

## Confusion matrix:
```{r}
cm <- confusionMatrix(factor(xgb_pred_class_test, levels = c("no", "yes")), 
                      factor(y_test, levels = c("no", "yes")), 
                      positive = "yes")
print(cm)

# Convert to dataframe for ggplot
cm_table <- as.data.frame(cm$table)
colnames(cm_table) <- c("Predicted", "Actual", "Freq")

ggplot(data = cm_table, aes(x = Actual, y = Predicted, fill = Freq)) +
  geom_tile(color = "white") +
  geom_text(aes(label = Freq), size = 6) +
  scale_fill_gradient(low = "white", high = "steelblue") +
  labs(title = "XGBoost Confusion Matrix", x = "Actual", y = "Predicted") +
  theme_minimal()
```

## ROC: 
```{r}
true_labels <- as.numeric(test_data2$amphet_use == "yes") # 1 for "yes", 0 for "no"

# ROC analysis
xgb_roc_obj <- roc(response = true_labels, predictor = xgb_pred_probs_test)

# Print AUC
xgb_auc <- auc(xgb_roc_obj)
print(xgb_auc)

# Plot ROC curve
plot(xgb_roc_obj, main = "XGBoost ROC Curve", col = "red")
```

# Summary of model performance
```{r}
model_names <- c("Logistic Regression", "Gradient Descent (Sigmoid)", "Lasso Regression" , "Random Forest", "XGBoost")
accuracy <- c(base_test_accuracy, sigmoid_test_accuracy, lasso_test_acc, rf_accuracy, xgb_test_accuracy)
auc <- c(log_auc, sigmoid_auc, lasso_auc, rf_auc, xgb_auc)

# Create a data frame
summary_table <- data.frame(
  Model = model_names,
  Accuracy = accuracy,
  AUC = auc
)

# View the table
print(summary_table)
```



