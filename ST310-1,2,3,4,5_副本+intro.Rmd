---
title: "ST310 Project"
date: "2025-04-11"
output: html_document
---

```{r setup1, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
rm(list = ls())
```

## Introduction

Drug consumption is known to be harmful, but many people still underestimate the risks. And the impact of drug use isn’t limited to personal health, it can also lead to real danger for others. In March 2025, a tragic accident occurred near King’s College London, where a student was killed and two others were injured after being hit by a van. The driver was arrested on suspicion of careless driving and drug-driving (BBC News, 2025).

In the UK, many drugs are classified as illegal such as cannabis, ecstasy, and cocaine. However, their usage persists across different age groups. Among all the drugs, cannabis is the most commonly used drug in England and Wales. According to the Crime Survey for England and Wales, around 6.8% of people aged 16 to 59 years reported using cannabis in the past 12 months, with an even higher rate of 13.8% among young adults aged 16 to 24 (Office for National Statistics, 2024).

In addition to the misuse of illegal drugs, the non-medical use of prescription drugs is also prohibited. For example, amphetamine is a stimulant that sometimes prescribed to treat conditions like ADHD or narcolepsy due to its effects on alertness and focus. However, misuse of amphetamine is associated with serious consequences, including addiction, cardiovascular problems, paranoia, and aggressive behavior. According to Figure 1, amphetamine is ranked among the ten most addictive drugs due to its high potential for psychological dependence and pleasure effects.

This project aims to use machine learning methods to predict the usage of amphetamine (*amphet_use*) based on a series of psychological, demographic, and behavioral variables. For demographic factors, we include age (*age*), gender (*gender*), education level (*education*), country of residence (*country*), and ethnicity (*ethnicity*). Psychological factors cover personality traits such as neuroticism (*nscore*), extraversion (*escore*), openness (*oscore*), agreeableness (*ascore*), conscientiousness (*cscore*), impulsivity (*impulsive*), and sensation-seeking (*ss*). Behavioral factors include consumption levels of other drugs: alcohol (*alcohol_level*), chocolate (*choc_level*), cannabis (*cannabis_level*), caffeine (*caff_level*), and nicotine (*nicotine_level*). Instead of including every drug variables from the original dataset, we selected these drugs due to their widespread use or relatively lower risk, which may help reveal broader patterns of drug consumption associated with amphetamine use. By identifying the key predictors of amphetamine use, the findings from this analysis can contribute to better strategies for prevention and early intervention.

![*Figure 1: The 10 most addictive drugs*](ST310%20Machine%20Learning/drug.png){width="333"}

### Load libraries and dataset

Load libraries:

```{r}
library(caret)
library(ggplot2)
library(xgboost)
library(caret)
library(Matrix)
library(ROCR)
library(tidymodels)
library(dplyr)
library(yardstick)
library(smotefamily)
library(pROC)
library(lightgbm)
library(ROSE)
library(tibble)
```

Load data:

```{r}
#data <- read.csv("data/drug_consumption.csv")
data <- read.csv("~/Desktop/ST310 Machine Learning/project/drug_consumption.csv")

# Keep only selected drug variables along with the original features
selected_vars <- c("age", "gender", "education", "country", "ethnicity", "nscore", 
                   "escore", "oscore","ascore", "cscore", "impuslive", "ss", 
                   "alcohol", "choc", "cannabis", "caff", "nicotine", "amphet")
drug_data <-  data[, selected_vars]

# Rename 'impuslive' to 'impulsive'
colnames(drug_data)[colnames(drug_data) == "impuslive"] <- "impulsive"

head(drug_data)
table(drug_data$amphet)
```

Check data structure:

```{r}
str(drug_data)
```

### Initial data manipulation

Convert all drug variables into ordered factor (0–6):

```{r}
drug_data2 <- drug_data
drug_vars <- c("alcohol", "choc", "cannabis", "caff", "nicotine", "amphet")

# Convert all to ordered numeric levels
for (var in drug_vars) {
  drug_data2[[paste0(var, "_level")]] <- as.numeric(factor(drug_data2[[var]], 
                                                           levels = c("CL0", "CL1", "CL2", "CL3", "CL4", "CL5", "CL6"))) - 1
}

# Remove the original categorical drug columns
drug_data2 <- drug_data2[ , !(names(drug_data2) %in% drug_vars)]
```

Convert *amphet* to binary factor:

-   0 = no use (CL0)

-   1 = used (CL1 to CL6)

```{r}
# Create binary target: 0 = no use, 1 = used
drug_data2$amphet_use <- ifelse(drug_data2$amphet_level > 0, 1, 0)
drug_data2$amphet_use <- as.factor(drug_data2$amphet_use)

# Remove amphet_level
drug_data2$amphet_level <- NULL

head(drug_data2)
```

Check data structure:

```{r}
str(drug_data2)
```

Split data into training and testing set:

```{r}
set.seed(38036)

train_indices <- createDataPartition(drug_data2$amphet_use, p = 0.8, list = FALSE)

train_data2 <- drug_data2[train_indices, ]
test_data2  <- drug_data2[-train_indices, ]

head(train_data2)
table(train_data2$amphet_use)
```

### 1. Baseline model: logistic regression

**1.1 Build Logistic regression model:**

```{r}
# Create a recipe
logit_recipe <- recipe(amphet_use ~ ., data = train_data2)

# Specify the logistic regression model
logit_spec <- 
  logistic_reg(mode = "classification") %>% 
  set_engine("glm")

# Combine recipe and model into a workflow
logit_workflow <- 
  workflow() %>%
  add_model(logit_spec) %>%
  add_recipe(logit_recipe)

# Fit the workflow to the data
logit_fit <- fit(logit_workflow, data = train_data2)

# Full summary
logit_fit %>% 
  extract_fit_parsnip() %>% 
  pluck("fit") %>% 
  summary()
```

The result from logistic regression indicate that the most significant predictors of amphet use are age, gender, ss (sensation seeking), alcohol_level, cannabis level, caff_level, and nicotine_level. This finding is consistent with behavioral expectations: individuals who are older, male sex, and exhibit higher levels of sensation seeking are more inclined to experiment with substances like Amphetamine. Additionally, individuals who consume less alcohol and those who use more cannabis, caffeine, and nicotine are more likely use Amphetamine.

**1.2 Compute accuracy:**

```{r}
# Predict on test data
pred_class_test <- predict(logit_fit, new_data = test_data2, type = "class")$.pred_class
base_test_accuracy <- mean(pred_class_test == test_data2$amphet_use)
cat("Logistic Regression Test Accuracy:", round(base_test_accuracy, 3), "\n")

# Predict on train data
pred_class_train <- predict(logit_fit, new_data = train_data2, type = "class")$.pred_class
base_train_accuracy <- mean(pred_class_train == train_data2$amphet_use)
cat("Logistic Regression Training Accuracy:", round(base_train_accuracy, 3), "\n")
```

**1.3 Confusion matrix:**

```{r}
results_test <- tibble(
  truth = test_data2$amphet_use,
  pred = predict(logit_fit, new_data = test_data2, type = "class")$.pred_class
)

# Compute the confusion matrix
cm <- conf_mat(results_test, truth = truth, estimate = pred)

# Plot the confusion matrix
autoplot(cm, type = "heatmap")
```

**1.4 Compute ROC:**

```{r}
true_labels <- as.numeric(as.character(test_data2$amphet_use))
pred_probs_test <- predict(logit_fit, new_data = test_data2, type = "prob")$.pred_1
roc_obj <- roc(response = true_labels, predictor = pred_probs_test)

# Print AUC
log_auc <- auc(roc_obj)
print(log_auc)

# Plot ROC curve
plot(roc_obj, main = "Logistic Regression ROC Curve", col = "blue")
```

The model's AUC is 0.80, indicating that there is a moderate 80% chance that the model ranks a random positive case (Amphetamine user) higher than a random negative case (non-user). Overall, the baseline model is performing relatively well with a test accuracy of 70%.

### 2. Custom Gradient Descent Model: Stochastic Gradient Descent (SGD) Model

We fitted the second model using our own implementation of stochastic gradient descent (SGD). We chose logistic regression as our model structure due to its simplicity and interpretability, and combined with the binary cross-entropy (BCE) loss function. This framework can keep the derivation and computation manageable, and better fit our binary classification task which predicting amphetamine use (amphet_use).

In our implementation, we first defined the sigmoid function to convert the linear combination of predictors into probabilities. Then, we calculated the gradient of the BCE loss, which depends on both the predicted probability and the true label. This satisfied the requirement that the gradient of the loss function should not be a constant because the gradient varies with each data point and changes during training.

We also applied mini-batch with a batch size of 32. In each epoch, the dataset was randomly shuffled and split into batches. For every mini-batch, we calculated the gradient, normalized it, and updated the weights (beta_hat). We tested several combinations of learning rates and number of epochs, and found out that a learning rate of 0.001 over 100 epochs had the most stable training and showed consistent learning. Our model aimed to minimize the BCE loss, which decreased from 0.6681 at epoch 1 to 0.5404 at epoch 100.

After training, we printed out the final weights, which indicated the importance and direction of each predictor related to the amphetamine use. For example:

-   Cannabis consumption (0.303) and age (0.269) had positive associations with amphetamine use, and were two largest values. In other words, amphetamine use is more likely among individuals who use cannabis more frequently, and those are relatively older.

-   Chocolate consumption (-0.168) and alcohol level (-0.141) showed negative associations with amphetamine use, which means that people who engage more frequently in these lower risk drugs are less reported to use amphetamines.

**2.1 Build Stochastic Gradient Descent Model:**

```{r}
predictors <- c("age", "gender", "education", "country", "ethnicity", 
                "nscore", "escore","oscore", "ascore", "cscore", "impulsive", "ss",
                "alcohol_level", "choc_level", "cannabis_level", "caff_level", "nicotine_level")
# Covert outcome variable to numeric
drug_data2$amphet_use <- as.numeric(as.character(drug_data2$amphet_use))


batch_size <- 32 # Mini-batch size

# Sigmoid Function:
sigmoid <- function(z) {
  1 / (1 + exp(-z)) # map linear output to probabilities
}

# Binary Cross-Entropy (BEC) Loss:
BCE_loss <- function(y_true, y_pred) {
  epsilon <- 1e-8
  -mean(y_true * log(y_pred + epsilon) + (1 - y_true) * log(1 - y_pred + epsilon))
}

# Logistic Gradient:
logistic_gradient <- function(y_true, y_pred, X) {
  t(X) %*% (y_pred - y_true) / nrow(X)
}


# Define Stochastic Gradient Descent:
sgd_logistic_regression <- function(train_data2, lr = 0.01, epochs = 100) {
  X <- as.matrix(drug_data2[, predictors])
  y <- drug_data2$amphet_use
  
  X <- cbind(Intercept = 1, X) # Add intercept term
  beta_hat <- rep(0, ncol(X)) # Initialize weights
  
  n <- nrow(X)
  loss_history <- numeric(epochs)
  num_batches <- ceiling(n/batch_size)
  
  for (epoch in 1:epochs) {
    shuffled_indices <- sample(1:n, n, replace = TRUE) # Shuffle data
    
    for (batch in 1:num_batches) {
      batch_start <- 1 + batch_size * (batch - 1)
      batch_end <- min(batch * batch_size, n)
      batch_indices <- shuffled_indices[batch_start:batch_end]
      # Mini-batch
      X_batch <- X[batch_indices, , drop = FALSE]
      y_batch <- y[batch_indices]
      
      # Predict probabilities using sigmoid
      y_pred_batch <- sigmoid(X_batch %*% beta_hat)
      # Compute gradient
      gradient_batch <- logistic_gradient(y_batch, y_pred_batch, X_batch)
      # Normalize gradient
      gradient_norm <- sqrt(sum(gradient_batch^2))
      # Update beta_hat (weights)
      beta_hat <- beta_hat - lr * gradient_batch / (1e-8 + gradient_norm)
  }
    y_pred <- sigmoid(X %*% beta_hat)
    epoch_loss <- BCE_loss(y, y_pred)
    loss_history[epoch] <- epoch_loss

    # Print out loss every 10 epochs
    if (epoch %% 10 == 0 || epoch == 1) {
      cat("Epoch:", epoch, "- Loss:", round(epoch_loss, 4), "\n")
}  
  }
  
  return(list(weights = beta_hat, loss_history = loss_history))
}

# Train the model
result <- sgd_logistic_regression(train_data2, lr = 0.001, epochs = 100)

# Extract final weights and loss history
final_weights <- result$weights
loss_history <- result$loss_history

# Display final weights
cat("Final Weights:\n")
print(final_weights)
```

```{r}
# Plot traing loss over epochs
plot(1:length(loss_history), loss_history,
     type = "b", pch = 19,
     xlab = "Epoch", ylab = "Loss (BCE)",
     main = "Training Loss over Epochs")
```

*Figure 2. Training Loss over Epochs using custom SGD implementation.* The BCE loss consistently decreases over the 100 epochs, reinforcing that our chosen learning rate (0.001) and number of epochs (100) led to a stable and convergent training process.

**2.2 Evaluate on Test and Train Data:**

```{r}
# Test Data:
X_test <- as.matrix(test_data2[, -ncol(test_data2)])
y_test <- as.numeric(as.character(y_test <- test_data2$amphet_use))
# Add intercept term
X_test <- cbind(Intercept = 1, X_test)

# Compute predictions
z_test <- X_test %*% final_weights
y_hat_test <- sigmoid(z_test)
predictions_test <- ifelse(y_hat_test >= 0.5, 1, 0)

# Calculate accuracy
SGD_accuracy_test <- mean(predictions_test == y_test)
cat("SGD Model Accuracy (Test Set):", round(SGD_accuracy_test, 3), "\n")


# Train Data:
X_train <- as.matrix(train_data2[, -ncol(train_data2)])
y_train <- as.numeric(as.character(y_train <- train_data2$amphet_use))
# Add intercept term
X_train <- cbind(Intercept = 1, X_train)

# Compute predictions
z_test2 <- X_train %*% final_weights
y_hat_test2 <- sigmoid(z_test2)
predictions_test2 <- ifelse(y_hat_test2 >= 0.5, 1, 0)

# Calculate accuracy
SGD_accuracy_train <- mean(predictions_test2 == y_train)
cat("SGD Model Accuracy (Train Set):", round(SGD_accuracy_train, 3), "\n")
```

**2.3 Confusion Matrix for SGD (Test Set):**

```{r}
results_test <- tibble(
  truth = factor(y_test, levels = c(0, 1)),
  pred = factor(predictions_test, levels = c(0, 1))
)

# Compute the confusion matrix
cm_SGD <- conf_mat(results_test, truth = truth, estimate = pred)

# Plot the confusion matrix
autoplot(cm_SGD, type = "heatmap") + ggtitle("Confusion Matrix - Test Set (SGD Model)") + theme_minimal()
```

**2.4 Compute ROC:**

```{r}
true_labels <- as.numeric(as.character(test_data2$amphet_use))
pred_probs_test <- y_hat_test
roc_obj <- roc(response = true_labels, predictor = pred_probs_test)

# Print AUC
SGD_auc <- auc(roc_obj)
print(SGD_auc)

# Plot ROC curve
plot(roc_obj, main = "SGD Model ROC Curve", col = "blue")
```

Our custom SGD model had an accuracy of 72.6% on the test set and 72.8% on the training set, which indicates this model does not have the overfitting problem. The ROC curve gave an AUC of 0.81, suggesting that the model can correctly distinguish between a user and a non-user of amphetamines about 81% of the time.

### 3. Interpretable Model: Lasso Regression

For our third relatively interpretable model, we used lasso regression to better understand what factors affect amphetamine use most. The lasso regression applied an L1 penalty to make the less important predictors more close to zero, which make it easier for us to identify the most significant predictors and interpret the results.

We used the structure of *tidymodels* to build our lasso regression, and performed a 10 folds cross-validation. After tuning, the best penalty value ($\lambda$) was $1\times10^{-10}$. We used this value to fit our final lasso regression model that included all predictors. The estimated coefficients of intercept and most significant predictors are as following (the significant predictors are sorted by magnitude from largest to smallest):

-   Intercept (-0.125): This is the baseline value when all the other predictors are set to zero.

-   Cannabis consumption (0.809): The strongest positive predictor in our lasso regression model. It indicates that individuals who use cannabis more frequently are much more likely to also use amphetamines.

-   Nicotine consumption (0.479): The nicotine consumption is positively associated with amphetamine use, suggesting that higher nicotine use is linked to higher likelihood of amphetamine use.

-   Age (0.427): Older individuals in this dataset are more likely to report the use of amphetamine.

-   Sensation-seeking (0.253): Individuals with higher sensation-seeking scores are more likely to use amphetamines.

-   Caffeine consumption (0.163): Individuals who consume more caffeine are also more likely to use amphetamines.

-   Gender (-0.150): There is a negative relationship between gender and amphetamine use. The negative coefficient implies that female individuals (coded as +0.482) are less likely to use amphetamines compared to males (coded as -0.482).

-   Alcohol level (-0.125): The alcohol consumption is negatively associated with amphetamine use, suggesting that people who drink alcohol more frequently show lower amphetamine use.

These seven significant predictors above consisted with the results from our baseline model, suggesting that psychological (sensation-seeking), demographic (age, gender), and behavioral (use of other drugs like cannabis, nicotine, caffeine, and alcohol) factors all contribute to amphetamine use. The results also supported that drug consumption behaviors are often interconnected, which means individuals who engage in one form of drug use are more likely to engage in others such as cannabis.

Compared to the baseline logistic model, this lasso regression model had the same test accuracy of 70.2% and train accuracy of 73.1%. The area under the ROC curve (AUC) was also similar (80.3%), and reinforced by the comparison ROC curves in Figure 3 which almost entirely overlapping. These findings showed that baseline logistic model and lasso regression model had similar classification ability, and applying L1 regularization did not improve predictive power. However, while both models are interpretable, the lasso model is easier for readers with less quantitative background to understand the most significant predictors because lasso regression reduced less important coefficients closer to zero instead of interpreting p-values or statistical significance tests.

**3.1 Build Lasso regression model:**

```{r}
# Convert outcome variable to factor
train_data2$amphet_use <- as.factor(train_data2$amphet_use)
test_data2$amphet_use <- as.factor(test_data2$amphet_use)

# Build Lasso Model
lasso_recipe <- recipe(amphet_use ~ ., data = train_data2) %>%
  step_normalize(all_predictors()) # normalize all predictors

lasso_spec <- logistic_reg(
  mode = "classification",
  penalty = tune(),       
  mixture = 1 # L1 penalty (lasso)
) %>%
  set_engine("glmnet")

# Cross-validation
set.seed(38036)
folds <- vfold_cv(train_data2, v = 10)

# Lasso workflow
lasso_workflow <- workflow() %>%
  add_recipe(lasso_recipe) %>%
  add_model(lasso_spec)

lasso_grid <- grid_regular(penalty(), levels = 30) # Create a grid of lambda (penalty) values

# Tune the model over the grid using cross-validation
tuned_lasso <- tune_grid(
  lasso_workflow,
  resamples = folds,
  grid = lasso_grid,
  metrics = metric_set(accuracy, roc_auc)
)

# Find the best lambda based on accuracy
best_lasso <- select_best(tuned_lasso, metric = "accuracy")
print(best_lasso)

# Fit the final lasso model with the best lambda
final_lasso <- finalize_workflow(lasso_workflow, best_lasso)
lasso_fit <- fit(final_lasso, data = train_data2)
```

**3.2 Evaluate on Test and Train Data:**

```{r}
# Test Data:
pred_test <- predict(lasso_fit, new_data = test_data2, type = "class") %>%
  pull(.pred_class)

# Predict probabilities 
probs_test <- predict(lasso_fit, new_data = test_data2, type = "prob")

# Test Accuracy
lasso_test_acc <- mean(pred_test == test_data2$amphet_use)
cat("Lasso Regression Accuracy (Test Set):", round(lasso_test_acc, 3), "\n")

# Train Data:
pred_train <- predict(lasso_fit, new_data = train_data2, type = "class") %>%
  pull(.pred_class)

# Train Accuracy
lasso_train_acc <- mean(pred_train == train_data2$amphet_use)
cat("Lasso Regression Accuracy (Train Set):", round(lasso_train_acc, 3), "\n")
```

```{r}
lasso_coef <- lasso_fit %>%
  extract_fit_parsnip() %>%
  tidy() %>%
  filter(estimate != 0) %>%
  arrange(desc(abs(estimate)))

print(lasso_coef)
```

**3.3 Confusion Matrix for Lasso (Test Set):**

```{r}
lasso_pred_class <- predict(lasso_fit, new_data = test_data2, type = "class") %>%
  bind_cols(test_data2)

cm_lasso <- conf_mat(lasso_pred_class, truth = amphet_use, estimate = .pred_class)
autoplot(cm_lasso, type = "heatmap") + ggtitle("Lasso Confusion Matrix - Test Set") +
  theme_minimal()
```

**3.4 Compute ROC & Compare with Baseline Logistic Model:**

```{r}
# Baseline logistic model predictions
log_probs <- predict(logit_fit, test_data2, type = "prob") %>%
  bind_cols(test_data2) %>%
  mutate(model = "Logistic")

# Lasso model predictions
lasso_probs <- predict(lasso_fit, test_data2, type = "prob") %>%
  bind_cols(test_data2) %>%
  mutate(model = "Lasso")

true_labels <- test_data2$amphet_use
pred_probs_lasso <- probs_test %>% pull(.pred_1)
# Print AUC
roc_lasso <- roc(response = true_labels, predictor = pred_probs_lasso)
lasso_auc <- auc(roc_lasso)
print(lasso_auc)

# Plot ROC comparison curve
all_probs <- bind_rows(log_probs, lasso_probs)

roc_df <- all_probs %>%
  group_by(model) %>%
  roc_curve(truth = amphet_use, .pred_1)

autoplot(roc_df) +
  ggtitle("ROC Curve Comparison: Logistic vs Lasso") +
  theme_minimal()
```

*Figure 3. ROC curve comparison between the baseline logistic model and the lasso regression model.* The two curves are nearly identical with AUC values around 0.803.

### 4. High Dimensional Model: Random Forest

4.  At least one model must be (relatively) high-dimensional. If your dataset has many predictors, and the number of observations is not much larger, then for example you could fit a penalized regression model using all the predictors. If your dataset does not have many predictors you could consider models that include non-linear transformations, interaction terms, and/or local smoothing to increase the effective degrees of freedom.

The high-dimensional model was a Random Forest classifier built to predict binary amphetamine use (use vs. no use). We first applied oversampling to the minority class in the training data to mitigate class imbalance. After tuning the number of variables tried at each split (mtry ∈ {2, 3, 4}) via 5-fold cross-validation, the final forest attained 73.4% accuracy on the held-out test set.

Examining the MeanDecreaseGini importances from the fitted forest, we found that other substance-use measures are the strongest predictors of amphetamine consumption. Cannabis_level was by far the top signal, followed by nicotine_level. Among personality dimensions, openness (oscore), conscientiousness (cscore) and neuroticism (nscore) emerged next in importance. Sensation-seeking (ss) and extraversion (escore) also contributed substantially, with agreeableness (ascore) trailing closely. Demographic features such as education, age and country had smaller—but still noticeable—effects. Together, these results suggest that patterns of other drug use, particularly cannabis and nicotine.

**Build Random Forest:**

```{r}
set.seed(38036)
cv_splits <- vfold_cv(train_data2, v = 5, strata = amphet_use)

```

```{r}
rf_recipe <- recipe(amphet_use ~ ., data = train_data2)

```

```{r}
rf_spec <- 
  rand_forest(
    trees = 100,
    mtry  = tune()
  ) %>%
  set_mode("classification") %>%
  set_engine("randomForest")

```

```{r}
rf_wf <- workflow() %>%
  add_recipe(rf_recipe) %>%
  add_model(rf_spec)

mtry_grid <- tibble(mtry = c(2L, 3L, 4L))

```

```{r}
rf_res <- tune_grid(
  rf_wf,
  resamples = cv_splits,
  grid      = mtry_grid,
  metrics   = metric_set(accuracy, roc_auc)
)

best_rf <- rf_res %>% select_best(metric = "accuracy")
```

```{r}
rf_final <- rf_wf %>%
  finalize_workflow(best_rf) %>%
  fit(data = train_data2)

```

```{r}
pred_train <- predict(rf_final, train_data2) %>% pull(.pred_class)
pred_test  <- predict(rf_final, test_data2)  %>% pull(.pred_class)

train_acc <- mean(pred_train == train_data2$amphet_use)
test_acc  <- mean(pred_test  == test_data2$amphet_use)

cat("RF Accuracy (Train):", round(train_acc,3), "\n")
cat("RF Accuracy (Test) :", round(test_acc,3), "\n")

```

```{r}
rf_eng <- extract_fit_engine(rf_final)
print(importance(rf_eng))
varImpPlot(rf_eng)
```

### 5. Predictive model: XGB

## code using tidymodels

```{r}
# basic recipe
xgb_recipe <- recipe(amphet_use ~ ., data = train_data2) %>%
  step_dummy(all_nominal_predictors())

# model specification
xgb_spec <- boost_tree(
  mode = "classification",
  trees = tune(),              # nrounds
  tree_depth = tune(),         # max_depth
  learn_rate = tune(),         # eta
  loss_reduction = tune(),     # gamma
  sample_size = tune(),        # subsample
  mtry = tune(),               # colsample_bytree
  min_n = tune()               # min_child_weight
) %>%
  set_engine("xgboost")

# workflow
xgb_workflow <- workflow() %>%
  add_recipe(xgb_recipe) %>%
  add_model(xgb_spec)

# cross-validation folds
set.seed(123)
cv_folds <- vfold_cv(train_data2, v = 10)

n_predictors <- ncol(train_data2) - 1
library(dials)
xgb_grid <- grid_latin_hypercube(
  trees(range = c(50, 200)),
  tree_depth(range = c(3, 10)),
  learn_rate(range = c(0.01, 0.3)),
  loss_reduction(range = c(0, 5)),
  sample_size(range = c(0.5, 1)),
  mtry(range = c(1, n_predictors)),
  min_n(range = c(1, 10)),
  size = 20
)

xgb_grid <- as_tibble(xgb_grid)

# tune the grid
set.seed(38036)
xgb_tune_results <- tune_grid(
  xgb_workflow,
  resamples = cv_folds,
  grid = xgb_grid,
  metrics = metric_set(roc_auc)
)

# get best hyperparameters
best_xgb <- select_best(xgb_tune_results, metric= "roc_auc")

# finalize workflow with best parameters
final_xgb_workflow <- finalize_workflow(xgb_workflow, best_xgb)

# fit final model to all training data
final_xgb_fit <- fit(final_xgb_workflow, data = train_data2)
```

## Prepare data:

```{r}
train_data2$amphet_use <- factor(train_data2$amphet_use, levels = c(0, 1), labels = c("no", "yes"))
test_data2$amphet_use  <- factor(test_data2$amphet_use,  levels = c(0, 1), labels = c("no", "yes"))

X_train <- train_data2[, setdiff(names(train_data2), "amphet_use")]
X_test <- test_data2[, setdiff(names(test_data2), "amphet_use")]
y_train <- train_data2$amphet_use
y_test  <- test_data2$amphet_use
```

## Define train control:

```{r}
ctrl <- trainControl(
  method = "cv",      
  number = 5,
  verboseIter = TRUE,
  classProbs = TRUE,   
  summaryFunction = twoClassSummary
)
```

## Parameter grid:

```{r}
grid <- expand.grid(
  nrounds = c(50, 100),
  max_depth = c(3, 6, 9),
  eta = c(0.01, 0.1, 0.3),
  gamma = 0,
  colsample_bytree = c(0.7, 1),
  min_child_weight = 1,
  subsample = c(0.7, 1)
)
```

## XGB model:

```{r}
set.seed(38036)

xgb_caret <- train(
  x = X_train,
  y = y_train,
  method = "xgbTree",
  trControl = ctrl,
  tuneGrid = grid,
  metric = "ROC"
)
```

```{r}
print(xgb_caret$bestTune)
plot(xgb_caret)
```

## Accuracy using tidymodels:

```{r}
# Predict probabilities on training data
pred_class_train <- predict(final_xgb_fit, new_data = train_data2, type = "class")$.pred_class
train_acc <- mean(pred_class_train == train_data2$amphet_use)
cat("XGBoost Training Accuracy:", round(train_acc, 3), "\n")

# Predict probabilities on test data
pred_class_test <- predict(final_xgb_fit, new_data = test_data2, type = "class")$.pred_class
test_acc <- mean(pred_class_test == test_data2$amphet_use)
cat("XGBoost Test Accuracy:", round(test_acc, 3), "\n")

```

## Accuracy:

```{r}
# Predict probabilities on test data
xgb_pred_probs_test <- predict(xgb_caret, newdata = test_data2, type = "prob")[, "yes"]
xgb_pred_class_test <- ifelse(xgb_pred_probs_test > 0.5, "yes", "no")
xgb_test_accuracy <- mean(xgb_pred_class_test == test_data2$amphet_use)
cat("XGBoost Test Accuracy:", round(xgb_test_accuracy, 3), "\n")

# Predict probabilities on training data
xgb_pred_probs_train <- predict(xgb_caret, newdata = train_data2, type = "prob")[, "yes"]
xgb_pred_class_train <- ifelse(xgb_pred_probs_train > 0.5, "yes", "no")
xgb_train_accuracy <- mean(xgb_pred_class_train == train_data2$amphet_use)
cat("XGBoost Training Accuracy:", round(xgb_train_accuracy, 3), "\n")
```

## Confusion matrix:

```{r}
cm <- confusionMatrix(factor(xgb_pred_class_test, levels = c("no", "yes")), 
                      factor(y_test, levels = c("no", "yes")), 
                      positive = "yes")
print(cm)

# Convert to dataframe for ggplot
cm_table <- as.data.frame(cm$table)
colnames(cm_table) <- c("Predicted", "Actual", "Freq")

ggplot(data = cm_table, aes(x = Actual, y = Predicted, fill = Freq)) +
  geom_tile(color = "white") +
  geom_text(aes(label = Freq), size = 6) +
  scale_fill_gradient(low = "white", high = "steelblue") +
  labs(title = "XGBoost Confusion Matrix", x = "Actual", y = "Predicted") +
  theme_minimal()
```

## ROC:

```{r}
true_labels <- as.numeric(test_data2$amphet_use == "yes") # 1 for "yes", 0 for "no"

# ROC analysis
xgb_roc_obj <- roc(response = true_labels, predictor = xgb_pred_probs_test)

# Print AUC
xgb_auc <- auc(xgb_roc_obj)
print(xgb_auc)

# Plot ROC curve
plot(xgb_roc_obj, main = "XGBoost ROC Curve", col = "red")
```

# Summary of model performance

```{r}
model_names <- c("Logistic Regression", "SGD", "Lasso Regression" , "Random Forest", "XGBoost")
accuracy <- c(base_test_accuracy, SGD_accuracy_test, lasso_test_acc, rf_accuracy, xgb_test_accuracy)
auc <- c(log_auc, SGD_auc, lasso_auc, rf_auc, xgb_auc)

# Create a data frame
summary_table <- data.frame(
  Model = model_names,
  Accuracy = accuracy,
  AUC = auc
)

# View the table
print(summary_table)
```

### Reference

-   BBC News. (2025, March 25). Student killed and two injured in Strand van crash named. <https://www.bbc.co.uk/news/articles/cpdeg86ppdjo>

-   Office for National Statistics. (2024, December 12). Drug misuse in England and Wales: year ending March 2024. <https://www.ons.gov.uk/peoplepopulationandcommunity/crimeandjustice/articles/drugmisuseinenglandandwales/yearendingmarch2024>
